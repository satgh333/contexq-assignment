name: Workflow3

on:
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  S3_BUCKET: ${{ secrets.S3_BUCKET }}

jobs:
  pipeline:
    runs-on: ubuntu-latest

    steps:
    # 1️⃣ Checkout repo
    - uses: actions/checkout@v4

    # 2️⃣ Set up Python
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    # 3️⃣ Set up Java (for PySpark)
    - name: Set up Java
      uses: actions/setup-java@v4
      with:
        distribution: 'temurin'
        java-version: '17'

    # 4️⃣ Install dependencies
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install pyspark mlflow pytest python-Levenshtein pandas numpy boto3

    # 5️⃣ Configure AWS credentials
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    # # 6️⃣ Prepare JSON data for Spark
    # - name: Fix JSON files
    #   run: python fix_json.py

    # # 7️⃣ Upload data to S3
    # - name: Upload data to S3
    #   run: aws s3 cp data/ s3://${{ env.S3_BUCKET }}/data/ --recursive

    # 8️⃣ Run the cloud pipeline (S3 + Spark)
    - name: Run cloud pipeline
      env:
        S3_BUCKET: ${{ env.S3_BUCKET }}
        AWS_REGION: ${{ env.AWS_REGION }}
        PYTHONPATH: ${{ github.workspace }}
      run: python src/cloud_pipeline.py

    # 9️⃣ Run tests with PYTHONPATH
    - name: Run unit tests
      env:
        PYTHONPATH: ${{ github.workspace }}
      run: pytest tests/ -v